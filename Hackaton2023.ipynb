{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTaRpeIxl8jQ"
      },
      "source": [
        "# Day 1\n",
        "##About the problem\n",
        "\n",
        "The problem will ask you to create a classification of legal documents representing laws in force in the European Union (EU) with respect to the directory they belong to.\n",
        "\n",
        "The \"Directory of legal acts\", i.e. the hierarchy of rules in force in the EU, includes 20 main chapters:\n",
        "\n",
        "General, financial and institutional matters;\n",
        "- Customs Union and free movement of goods;\n",
        "- Agriculture;\n",
        "- Fisheries:\n",
        "[â€¦]\n",
        "\n",
        "Each chapter is divided into sub-chapters at various levels. The chapters have been coded with integers from 1 to 20 and represent the target of the classification, which must process the text of the law and/or the normative references and predict the chapter to which it belongs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJyNvve6pozX"
      },
      "outputs": [],
      "source": [
        "#Import packages\n",
        "import csv\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PMYhtUADroaP",
        "outputId": "74ea0ab4-78cb-4713-abe1-236743118226"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsDhRHEzr3Cu",
        "outputId": "0f757526-128d-455c-a6c8-d6db615abdf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1btDOKEr_Rz"
      },
      "outputs": [],
      "source": [
        "#Mount Sara\n",
        "os.getcwd()\n",
        "path = '/content/drive/MyDrive/hackathon2023_UNIMI/day1'\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b85mGkKUuB4z"
      },
      "outputs": [],
      "source": [
        "#Mount Giacomo\n",
        "os.getcwd()\n",
        "path = \"/content/drive/MyDrive/hackathon2023_UNIMI/day1\"\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu4YXyEjuCMS"
      },
      "outputs": [],
      "source": [
        "#Mount Doina\n",
        "os.getcwd()\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/hackathon2023_UNIMI/day1'\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK-_PkNWuCVj"
      },
      "outputs": [],
      "source": [
        "#Mount Konstantin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tZ0WNQHSsrfx",
        "outputId": "a3d3949b-d6db-429b-c3fc-23619131a89e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/hackathon2023_UNIMI/day1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHjUicfTT1vW"
      },
      "source": [
        "## Creation of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EgA8tJlpzl4",
        "outputId": "45881e73-4629-4910-d073-d825b457e83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory code\n",
            "19\n"
          ]
        }
      ],
      "source": [
        "#Opening the train csv and the test csv\n",
        "train = open(\"train/train_set.csv\")\n",
        "reader_train = csv.reader(train,delimiter=\",\")\n",
        "test = open(\"test/new_test.csv\")\n",
        "reader_test = csv.reader(test,delimiter=\",\")\n",
        "\n",
        "maxInt = sys.maxsize\n",
        "while True:\n",
        "  try:\n",
        "    csv.field_size_limit(maxInt)\n",
        "    break\n",
        "  except OverflowError:\n",
        "    maxInt = int(maxInt/10)\n",
        "\n",
        "count = 0\n",
        "for row in reader_train:\n",
        "  if count > 1:\n",
        "    break\n",
        "  print(row[2])\n",
        "  count += 1\n",
        "\n",
        "#count = 0\n",
        "#for row in spamreader:\n",
        "#  print(row)\n",
        "#  count += 1\n",
        "#  if count > 0:\n",
        "#    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7ybT9DC7iSP"
      },
      "outputs": [],
      "source": [
        "#Splitting training documents in the correct directory\n",
        "counter = 0\n",
        "\n",
        "array1 = []\n",
        "array2 = []\n",
        "array3 = []\n",
        "array4 = []\n",
        "array5 = []\n",
        "array6 = []\n",
        "array7 = []\n",
        "array8 = []\n",
        "array9 = []\n",
        "array10 = []\n",
        "array11 = []\n",
        "array12 = []\n",
        "array13 = []\n",
        "array14 = []\n",
        "array15 = []\n",
        "array16 = []\n",
        "array17 = []\n",
        "array18 = []\n",
        "array19 = []\n",
        "array20 = []\n",
        "\n",
        "#Opening the train csv and the test csv\n",
        "train = open(\"train/train_set.csv\")\n",
        "reader_train = csv.reader(train,delimiter=\",\")\n",
        "test = open(\"test/new_test.csv\")\n",
        "reader_test = csv.reader(test,delimiter=\",\")\n",
        "\n",
        "for row in reader_train:\n",
        "\n",
        "  if counter == 0:\n",
        "    counter += 1\n",
        "    continue\n",
        "\n",
        "  directory_code = row[2]\n",
        "  text = row[1]\n",
        "  document_id = row[0]\n",
        "\n",
        "  if directory_code == \"01\":\n",
        "    array1.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"02\":\n",
        "    array2.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"03\":\n",
        "    array3.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"04\":\n",
        "    array4.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"05\":\n",
        "    array5.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"06\":\n",
        "    array6.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"07\":\n",
        "    array7.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"08\":\n",
        "    array8.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"09\":\n",
        "    array9.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"10\":\n",
        "    array10.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"11\":\n",
        "    array11.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"12\":\n",
        "    array12.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"13\":\n",
        "    array13.append([document_id,text])\n",
        "\n",
        "  if directory_code == \"14\":\n",
        "    array14.append([document_id, text])\n",
        "\n",
        "  if directory_code == \"15\":\n",
        "    array15.append([document_id, text])\n",
        "\n",
        "  if directory_code == \"16\":\n",
        "    array16.append([document_id, text])\n",
        "\n",
        "  if directory_code == \"17\":\n",
        "    array17.append([document_id, text])\n",
        "\n",
        "  if directory_code == \"18\":\n",
        "    array18.append([document_id, text])\n",
        "\n",
        "  if directory_code == \"19\":\n",
        "    array19.append([document_id, text])\n",
        "\n",
        "  if directory_code == \"20\":\n",
        "    array20.append([document_id, text])\n",
        "\n",
        "  counter += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_Aj2E0LJ-jp"
      },
      "outputs": [],
      "source": [
        "#TO PROCESS ONLY WHEN WE NEED TO CREATE THE DATASET\n",
        "count = 0\n",
        "\n",
        "for tupla in array1:\n",
        "  path1 = path+\"/train/1\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array2:\n",
        "  path1 = path+\"/train/2\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array3:\n",
        "  path1 = path+\"/train/3\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array4:\n",
        "  path1 = path+\"/train/4\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array5:\n",
        "  path1 = path+\"/train/5\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array6:\n",
        "  path1 = path+\"/train/6\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array7:\n",
        "  path1 = path+\"/train/7\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array8:\n",
        "  path1 = path+\"/train/8\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(\"Entrato\")\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array9:\n",
        "  path1 = path+\"/train/9\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array10:\n",
        "  path1 = path+\"/train/10\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array11:\n",
        "  path1 = path+\"/train/11\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array12:\n",
        "  path1 = path+\"/train/12\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array13:\n",
        "  path1 = path+\"/train/13\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array14:\n",
        "  path1 = path+\"/train/14\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array15:\n",
        "  path1 = path+\"/train/15\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array16:\n",
        "  path1 = path+\"/train/16\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array17:\n",
        "  path1 = path+\"/train/17\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array18:\n",
        "  path1 = path+\"/train/18\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array19:\n",
        "  path1 = path+\"/train/19\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "for tupla in array20:\n",
        "  path1 = path+\"/train/20\"\n",
        "  os.chdir(path1)\n",
        "  #with open(tupla[0]+\".txt\", \"w\") as file:\n",
        "  with open(str(count)+\".txt\", \"w\") as file:\n",
        "    print(tupla[0])\n",
        "    file.write(tupla[1])\n",
        "  count += 1\n",
        "\n",
        "print (\"FILE SCRITTI: \" + str(count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRQjYrYEx7bd"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlu_gmycxlVm"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeAJdulLr8rx"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSujTIsax5EW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf928934-6acb-493e-b9ee-d69719156976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNpzp9hGyrqM"
      },
      "outputs": [],
      "source": [
        "# dataset = os.getcwd() + \"\\dataset_frasi_discriminatorie\\\\dataset_ing\\\\train\"\n",
        "# dataset_dir = os.path.join(os.path.dirname(dataset), 'dataset_ing')\n",
        "# train_dir = os.path.join(dataset_dir, 'train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i7Jiq-nhOlA"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/hackathon2023_UNIMI/day1\"\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSMbv3rez6eA",
        "outputId": "47f1eb2c-80d1-4b60-db0a-44fffa4d8905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 57888 files belonging to 20 classes.\n",
            "Using 46311 files for training.\n"
          ]
        }
      ],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "#creazione del dataset etichettato utilizzando 'text_dataset_from_directory'\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvPGWc9MzNGA",
        "outputId": "39897ccc-4758-4e17-c373-6ca47fbccbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 57888 files belonging to 20 classes.\n",
            "Using 11577 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SEOeP-10Jak",
        "outputId": "75d8e4cc-617a-483f-dcc4-57e005294ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b'30.3.2010 EN Official Journal of the European Union L 83/19 COUNCIL DECISION of 16 March 2010 amending Decision 2009/459/EC providing Community medium-term financial assistance for Romania (2010/183/EU) THE COUNCIL OF THE EUROPEAN UNION, Having regard to the Treaty on the Functioning of the European Union, Having regard to Council Regulation (EC) No 332/2002 of 18 February 2002 establishing a facility providing medium-term financial assistance for Member States balances of payments(1) and in particular Article 5, second subparagraph, in conjunction with Article 8 thereof, Having regard to the proposal from the Commission made after consulting the Economic and Financial Committee (EFC), Whereas: (1) By Decision 2009/458/EC(2), the Council granted mutual assistance to Romania and by Decision 2009/459/EC(3), the Council provided medium-term financial assistance for Romania. (2) The scope and intensity of the economic recession affecting Romania calls for a revision of the economic policy conditions foreseen for the disbursement of the instalments of the financial assistance with a view to taking into account the impact of the larger-than-expected contraction of real GDP. (3) Decision 2009/459/EC should therefore be amended accordingly, HAS ADOPTED THIS DECISION: Article 1 Decision 2009/459/EC is hereby amended as follows: 1. in Article 3(5), point (a) shall be replaced by the following: (a) implementing a clearly set medium-term fiscal programme so as to bring the general government deficit below the Treaty reference value of 3% of GDP with a time-frame and a consolidation path which are consistent with the Council recommendations to Romania adopted under the excessive deficit procedure.; 2. in Article 3(5), point (b) shall be replaced by the following: (b) adopting and implementing annual budgets for 2010 and beyond, consistent with the consolidation path set out in the Supplemental Memorandum of Understanding.. Article 2 This Decision shall take effect on the day of its notification. Article 3 This Decision is addressed to Romania. Article 4 This Decision shall be published in the Official Journal of the European Union. Done at Brussels, 16 March 2010. \\nFor the Council\\n \\nThe President\\n E. SALGADO \\n(1)OJ L53, 23.2.2002, p.1. \\n(2)OJ L150, 13.6.2009, p.6. \\n(3)OJ L150, 13.6.2009, p.8.'\n",
            "Label : 1 (10)\n",
            "Review: b'23.12.2022 EN Official Journal of the European Union L 330/233 COMMISSION IMPLEMENTING DECISION (EU) 2022/2570 of 24November 2022 not approving silver nitrate as an active substance for use in biocidal products of product-type 7 in accordance with Regulation (EU) No528/2012 of the European Parliament and of the Council (Text with EEA relevance) THE EUROPEAN COMMISSION, Having regard to the Treaty on the Functioning of the European Union, Having regard to Regulation (EU) No528/2012 of the European Parliament and of the Council of 22May 2012 concerning the making available on the market and use of biocidal products(1), and in particular Article9(1), point (b), thereof, Whereas: (1) Pursuant to Article11(1) of Directive 98/8/EC of the European Parliament and of the Council(2), an application for approval of silver nitrate for use in biocidal products of product-type 7, film preservatives, as described in Annex V of that Directive, corresponding to product-type 7, film preservatives, as described in Annex V to Regulation (EU) No528/2012, was submitted to the competent authority of Sweden on 23December 2010. (2) Pursuant to Article90(2), first subparagraph, of Regulation (EU) No528/2012, applications submitted for the purposes of Directive 98/8/EC for which the Member States evaluation in accordance with Article11(2) of Directive 98/8/EC has not been completed by 1September 2013 are to be evaluated by the competent authorities in accordance with the provisions of that Regulation. (3) On 10February 2022, during the evaluation of the active substance by the evaluating competent authority, the applicant withdrew its application and no longer requests the approval of silver nitrate as an active substance for use in biocidal products of product-type 7. (4) Silver nitrate is not included for product-type 7 in Annex II to Commission Delegated Regulation (EU) No1062/2014(3), which lists the active substance/product-type combinations included in the work programme for the examination of existing biocidal active substances contained in biocidal products. Biocidal products of product-type 7 containing silver nitrate are therefore not covered by the transitional provisions laid down in Article89(2) of Regulation (EU) No528/2012 and may therefore not be made available or used on the Union market. (5) However, in accordance with the transitional provision set out in Article94(1), point (a), of Regulation (EU) No528/2012, a treated article treated with or intentionally incorporating one or more biocidal products containing only active substances that are under examination for the relevant product-type in the work programme referred to in Article89(1) of that Regulation on 1September 2016 or for which an application for approval for the relevant product-type is submitted by that date, or containing only a combination of such substances and active substances included in the list drawn up in accordance with Article9(2) of that Regulation for the relevant product-type and use or included in Annex I, may be placed on the market until the date falling 180 days after a decision not to approve one of the active substances for the relevant use, when such decision is adopted after 1September 2016. (6) As the applicant has withdrawn the application for approval of silver nitrate for use in biocidal products of product-type 7, there is no biocidal product to be evaluated. Consequently, the competent authority did not finalise the assessment report and the European Chemicals Agency did not prepare an opinion. Finally, as there is no biocidal product of product-type 7 containing silver nitrate that may be expected to meet the criteria laid down in Article19(1), point (b), of Regulation (EU) No528/2012, the conditions laid down in Article4(1) of that Regulation are not met. Considering also the need to ensure that treated articles treated with or intentionally incorporating silver nitrate for product-type 7 are no longer placed on the Union market, it is appropriate not to approve silver nitrate for use in biocidal products of product-type 7. (7) The measures provided for in this Decision are in accordance with the opinion of the Standing Committee on Biocidal Products, HAS ADOPTED THIS DECISION: Article1 Silver nitrate (EC No: 231-853-9; CAS No: 7761-88-8) is not approved as an active substance for use in biocidal products of product-type 7. Article2 This Decision shall enter into force on the twentieth day following that of its publication in the Official Journal of the European Union. Done at Brussels, 24November 2022. \\nFor the Commission\\n \\nThe President\\n Ursula VON DER LEYEN \\n(1)OJL167, 27.6.2012, p.1. \\n(2)Directive 98/8/EC of the European Parliament and of the Council of 16February 1998 concerning the placing of biocidal products on the market (OJL123, 24.4.1998, p.1). \\n(3)Commission Delegated Regulation (EU) No1062/2014 of 4August 2014 on the work programme for the systematic examination of all existing active substances contained in biocidal products referred to in Regulation (EU) No528/2012 of the European Parliament and of the Council (OJL294, 10.10.2014, p.1).'\n",
            "Label : 4 (13)\n",
            "Review: b'15.6.2011 EN Official Journal of the European Union L 156/7 DECISION No 1/2011 OF THE EU-CROATIA STABILISATION AND ASSOCIATION COUNCIL of 5 May 2011 amending Protocol 4 to the Stabilisation and Association Agreement between the European Communities and their Member States, of the one part, and the Republic of Croatia, of the other part, concerning the definition of the concept of originating products and methods of administrative cooperation (2011/340/EU) THE EU-CROATIA STABILISATION AND ASSOCIATION COUNCIL, Having regard to the Stabilisation and Association Agreement between the European Communities and their Member States, of the one part, and the Republic of Croatia, of the other part(1) (hereinafter referred to as the Agreement), and in particular Article 39 of Protocol 4 thereto, Whereas: (1) Articles 3 and 4 of Protocol 4 to the Agreement provide for bilateral cumulation of origin in the European Union or in Croatia. (2) Croatia requested to cumulate origin when incorporating materials originating in the Union, in Croatia or in any country or territory participating in the Unions Stabilisation and Association process(2) or incorporating the materials originating in Turkey to which the Decision No 1/95 of the EC-Turkey Association Council of 22 December 1995 on implementing the final phase of the Customs Union(3) applies(4). (3) In order to allow the Union and Croatia to benefit from the extended cumulation zone, the provisions of Protocol 4 to the Agreement should be amended accordingly, HAS ADOPTED THIS DECISION: Article 1 Protocol 4 to the Agreement is hereby amended as follows: (1) the following is added to the Table of Contents: Annex V: Products excluded from the cumulation provided for in Article 3 and Article 4; (2) Article 3 is replaced by the following: Article 3 Cumulation in the Community 1.Without prejudice to the provisions of Article 2(1), products shall be considered as originating in the Community if such products are obtained there, incorporating materials originating in Croatia, in the Community or in any country or territory participating in the European Unions Stabilisation and Association process(5), or incorporating the materials originating in Turkey to which the Decision No 1/95 of the EC-Turkey Association Council of 22 December 1995 applies(6), provided that the working or processing carried out in the Community goes beyond the operations referred to in Article 7. It shall not be necessary for such materials to have undergone sufficient working or processing. 2.Where the working or processing carried out in the Community does not go beyond the operations referred to in Article 7, the product obtained shall be considered as originating in the Community only where the value added there is greater than the value of the materials used originating in any one of the other countries or territories referred to in paragraph 1. If this is not so, the product obtained shall be considered as originating in the country which accounts for the highest value of originating materials used in the manufacture in the Community. 3.Products, originating in one of the countries or territories referred to in paragraph 1, which do not undergo any working or processing in the Community, retain their origin if exported into one of these countries or territories. 4.The cumulation provided for in this Article may be applied only provided that: (a) a preferential trade agreement in accordance with Article XXIV of the General Agreement on Tariffs and Trade (GATT) is applicable between the countries or territories involved in the acquisition of the originating status and the country of destination; (b) materials and products have acquired originating status by the application of rules of origin identical to those given in this Protocol; and (c) notices indicating the fulfilment of the necessary requirements to apply cumulation have been published in the Official Journal of the European Union (C series) and in Croatia according to its own procedures. The cumulation provided for in this Article shall apply from the date indicated in the notice published in the Official Journal of the European Union (C series). The Community shall provide Croatia, through the European Commission, with details of the Agreements and their corresponding rules of origin, which are applied with the other countries or territories referred to in paragraph 1. The products in Annex V shall be excluded from the cumulation provided for in this Article. (3) Article 4 is replaced by the following: Article 4 Cumulation in Croatia 1.Without prejudice to the provisions of Article 2(2), products shall be considered as originating in Croatia if such products are obtained there, incorporating materials originating in the Community, Croatia or in any country or territory participating in the European Unions Stabilisation and Association process(7), or incorporating the materials originating in Turkey to which the Decision No 1/95 of the EC-Turkey Association Council of 22 December 1995 applies(8), provided that the working or processing carried out in Croatia goes beyond the operations referred to in Article 7. It shall not be necessary for such materials to have undergone sufficient working or processing. 2.Where the working or processing carried out in Croatia does not go beyond the operations referred to in Article 7, the product obtained shall be considered as originating in Croatia only where the value added there is greater than the value of the materials used originating in any one of the other countries or territories referred to in paragraph 1. If this is not so, the product obtained shall be considered as originating in the country which accounts for the highest value of originating materials used in the manufacture in Croatia. 3.Products, originating in one of the countries or territories referred to in paragraph 1, which do not undergo any working or processing in Croatia, retain their origin if exported into one of these countries or territories. 4.The cumulation provided for in this Article may be applied only provided that: (a) a preferential trade agreement in accordance with Article XXIV of the General Agreement on Tariffs and Trade (GATT) is applicable between the countries or territories involved in the acquisition of the originating status and the country of destination; (b) materials and products have acquired originating status by the application of rules of origin identical to those given in this Protocol; and (c) notices indicating the fulfilment of the necessary requirements to apply cumulation have been published in the Official Journal of the European Union (C series) and in Croatia according to its own procedures. The cumulation provided for in this Article shall apply from the date indicated in the notice published in the Official Journal of the European Union (C series). Croatia shall provide the Community, through the European Commission, with details of the Agreements, including their dates of entry into force, and their corresponding rules of origin, which are applied with the other countries or territories referred to in paragraph 1. The products in Annex V shall be excluded from the cumulation provided for in this Article. (4) Article 7(1)(m) is replaced by the following: (m) simple mixing of products, whether or not of different kinds; mixing of sugar with any other material;; (5) Article 13(1) is replaced by the following: 1.The preferential treatment provided for under the Agreement applies only to products, satisfying the requirements of this Protocol, which are transported directly between the Community and Croatia or through the territories of the other countries or territories referred to in Articles 3 and 4. However, products constituting one single consignment may be transported through other territories with, should the occasion arise, trans-shipment or temporary warehousing in such territories, provided that they remain under the surveillance of the customs authorities in the country of transit or warehousing and do not undergo operations other than unloading, reloading or any operation designed to preserve them in good condition. Originating products may be transported by pipeline across territory other than that of the Community or Croatia.; (6) Article 14(1) is replaced by the following: 1.Originating products, sent for exhibition in a country or territory other than those referred to in Articles 3 and 4 and sold after the exhibition for import into the Community or into Croatia shall benefit on import from the provisions of the Agreement provided it is shown to the satisfaction of the customs authorities that: (a) an exporter has consigned these products from the Community or from Croatia to the country in which the exhibition is held and has exhibited them there; (b) the products have been sold or otherwise disposed of by that exporter to a person in the Community or in Croatia; (c) the products have been consigned during the exhibition or immediately thereafter in the state in which they were sent for exhibition; and (d) the products have not, since they were consigned for exhibition, been used for any purpose other than demonstration at the exhibition.; (7) Article 15(1) is replaced by the following: 1.Non-originating materials used in the manufacture of products originating in the Community, in Croatia or in one of the other countries or territories referred to in Articles 3 and 4 for which a proof of origin is issued or made out in accordance with the provisions of Title V shall not be subject in the Community or in Croatia to drawback of, or exemption from, customs duties of whatever kind.; (8) Article 17(4) is replaced by the following: 4.A movement certificate EUR.1 shall be issued by the customs authorities of a Member State of the Community or of Croatia if the products concerned can be considered as products originating in the Community, in Croatia or in one of the other countries or territories referred to in Articles 3 and 4 and fulfil the other requirements of this Protocol.; (9) Article 22(2) is replaced by the following: 2.An invoice declaration may be made out if the products concerned can be considered as products originating in the Community, in Croatia or in one of the other countries or territories referred to in Articles 3 and 4 and fulfil the other requirements of this Protocol.; (10) Article 28 is replaced by the following: Article 28 Supporting documents The documents referred to in Articles 17(3) and 22(3) used for the purpose of proving that products covered by a movement certificate EUR.1 or an invoice declaration can be considered as products originating in the Community, in Croatia or in one of the other countries or territories referred to in Articles 3 and 4 and fulfil the other requirements of this Protocol may consist, inter alia, of the following: (a) direct evidence of the processes carried out by the exporter or supplier to obtain the goods concerned, contained for example in the accounts or internal book-keeping of the exporter or supplier; (b) documents proving the originating status of materials used, issued or made out in the Community or in Croatia where these documents are used in accordance with domestic law; (c) documents proving the working or processing of materials in the Community or in Croatia, issued or made out in the Community or in Croatia, where these documents are used in accordance with domestic law; (d) movement certificates EUR.1 or invoice declarations proving the originating status of materials used, issued or made out in the Community or in Croatia in accordance with this Protocol, or in one of the other countries or territories referred to in Articles 3 and 4, in accordance with rules of origin which are identical to the rules in this Protocol; (e) appropriate evidence concerning working or processing undergone outside the Community or Croatia by application of Article 12, proving that the requirements of that Article have been satisfied.; (11) Article 31(1) is replaced by the following: 1.For the application of the provisions of Article 22(1)(b) and Article 27(3) in cases where products are invoiced in a currency other than euro, amounts in the national currencies of the Member States of the Community, of Croatia and of the other countries or territories referred to in Articles 3 and 4 equivalent to the amounts expressed in euro shall be fixed annually by each of the countries concerned.; (12) The Annex to this Decision is added to Protocol 4 of the Agreement as Annex V. Article 2 This Decision shall enter into force on the first day of the first month following the date of its adoption. Done at Brussels, 5 May 2011. \\nFor the EU-Croatia Stabilisation and Association Council\\n \\nThe President\\n C. ASHTON \\n(1)OJ L 26, 28.1.2005, p. 3. \\n(2)As defined in the Conclusions of the General Affairs Council of 29 April 1997 and the Communication from the Commission to the Council and to the European parliament of 26 May 1999 on the Stabilisation and Association process for countries of South-Eastern Europe  Bosnia and Herzegovina, Croatia, Federal Republic of Yugoslavia, former Yugoslav Republic of Macedonia and Albania. \\n(3)OJ L 35, 13.2.1996, p. 1. \\n(4)Decision No 1/95 of the EC-Turkey Association Council of 22 December 1995 on implementing the final phase of the Customs Union applies to products other than agricultural products as defined in the Agreement establishing an Association between the European Economic Community and Turkey (OJ 217, 29.12.1964, p. 3687/64), and other than coal and steel products as defined in the Agreement between the European Coal and Steel Community and the Republic of Turkey on trade in products covered by the Treaty establishing the European Coal and Steel Community (OJ L 227, 7.9.1996, p. 3). \\n(5)As defined in the Conclusions of the General Affairs Council in April 1997 and Commission Communication of May 1999 on the establishment of the Stabilisation and Association process with Western Balkan countries. \\n(6)Decision No 1/95 of the EC-Turkey Association Council of 22 December 1995 applies to products other than agricultural products as defined in the Agreement establishing an Association between the European Economic Community and Turkey and other than coal and steel products as defined in the Agreement between the European Coal and Steel Community and the Republic of Turkey on trade in products covered by the Treaty establishing the European Coal and Steel Community.; \\n(7)As defined in the Conclusions of the General Affairs Council in April 1997 and Commission Communication of May 1999 on the establishment of the Stabilisation and Association process with Western Balkan countries. \\n(8)Decision No 1/95 of the EC-Turkey Association Council of 22 December 1995 applies to products other than agricultural products as defined in the Agreement establishing an Association between the European Economic Community and Turkey and other than coal and steel products as defined in the Agreement between the European Coal and Steel Community and the Republic of Turkey on trade in products covered by the Treaty establishing the European Coal and Steel Community.; ANNEX ANNEX V \\nPRODUCTS EXCLUDED FROM THE CUMULATION PROVIDED FOR IN ARTICLE 3 AND ARTICLE 4\\n CN-Code Description 17049099 Other sugar confectionery, not containing cocoa 18061030 18061090 Chocolate and other food preparations containing cocoa  cacao powder, containing added sugar or sweetening matter:  containing 65% or more but less than 80% by weight of sucrose (including invert sugar expressed as sucrose) or isoglucose expressed as sucrose  containing 80% or more by weight of sucrose (including invert sugar expressed as sucrose) or isoglucose expressed as sucrose 18062095  Other food preparations containing cocoa in block, slabs or bars weighting more than 2kg or in liquid, paste, powder, granular or other bulk form in containers or immediate packaging of a content exceeding 2kg  Other  Other 19019099 Malt extract, food preparations of flour, groats, meal, starch or malt extract, not containing cocoa or containing less than 40% by weight of cocoa calculated on a totally defatted basis, not elsewhere specified or included, food preparations of goods of headings 0401 to 0404, not containing cocoa or containing less than 5% by weight of cocoa calculated on a totally defatted basis, not elsewhere specified or included  Other  Other (than malt extract)  Other 21011298 Other preparations with a basis of coffee 21012098 Other preparations with a basis of tea or mate 21069059 Food preparations not elsewhere specified or included  Other  Other 21069098 Food preparations not elsewhere specified or included:  Other (than protein concentrates and textured protein substances)  Other  Other 33021029 Mixtures of odoriferous substances and mixtures (including alcoholic solutions) with a basis of one or more of these substances, of a kind used as raw materials in industry; other preparations based on odoriferous substances, of a kind used for the manufacture of beverages:  Of a kind used in the food or drink industries  Of the type used in the drink industries:  Preparations containing all flavouring agents characterising a beverage:  Of an actual alcoholic strength by volume exceeding 0,5%  Other:  Containing no milkfats, sucrose, isoglucose, glucose, or starch or containing, by weight, less than 1,5% milkfat, 5% sucrose or isoglucose, 5% glucose or starch  Other'\n",
            "Label : 10 (19)\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(f'Review: {text_batch.numpy()[i]}')\n",
        "    label = label_batch.numpy()[i]\n",
        "    print(f'Label : {label} ({class_names[label]})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhUdgAthWMm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e7e7e7-084f-45cf-c62b-9d6d71df7cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_cased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "_7b1cmoTzz9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "3YJZKO0Ez7lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  #layer da utilizzare come punto di ingresso in una rete\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  #layer per la pre elaborazione\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "\n",
        "  #modello bert\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "\n",
        "  #livello dense\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  #livello drop out\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "jSeiWAi-0TNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "metadata": {
        "id": "aaEHQW780hIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calcola la perdita di entropia incrociata tra etichette vere e previste\n",
        "# 'from_logits=True' => la probabilitÃ  va da -inf a +inf\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "#calcola la frequenza con cui le previsioni corrispondono alle etichette binarie\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "EoLlX3Za00Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#numero cicli\n",
        "epochs = 5\n",
        "\n",
        "#restituisce il numero di cardinalitÃ  -> step da fare per ciclo\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "print('steps_per_epoch:', steps_per_epoch)\n",
        "\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "print('num_train_steps:', num_train_steps)\n",
        "\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "print('num_warmup_step:', num_warmup_steps)\n",
        "\n",
        "#tasso si apprendimento\n",
        "init_lr = 3e-5\n",
        "\n",
        "#creazione ottimizzatore\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, #tasso di apprendimento\n",
        "                                          num_train_steps=num_train_steps, #numero di train\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw') #tipo di ottimizzatore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am0GD0rT0_qI",
        "outputId": "149dcdb8-479c-4d4a-ec16-928bd5e86742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "steps_per_epoch: 1448\n",
            "num_train_steps: 7240\n",
            "num_warmup_step: 724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "QNLuEbu71az7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3EB2AKX1i66",
        "outputId": "28f6365d-5e42-4d20-fd7b-0190d3fbc90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
            "Epoch 1/5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}